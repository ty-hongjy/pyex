1、技术路径：采用三种分类方式进行训练，K近邻分类器(KNN)，决策树(DecisionTree)，朴素贝叶斯(GussianNB)
	KNN：通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前K个点，根据“少数服从	多数“的原则，将这个数据点划分为出现次数最多的那个类别。
	在实际使用时，我们可以使用所有训练数据构成特征 X 和标签 y，使用fit()	函数进行训练。在正式分类时，通过一次性构造测试集或者一个一个输入样本的方式，得到样本对应的分类结果。
	有关K 的取值：
	• 如果较大，相当于使用较大邻域中的训练实例进行预测，可以减小估计误差，
	但是距离较远的样本也会对预测起作用，导致预测错误。
	• 相反地，如果 K 较小，相当于使用较小的邻域进行预测，如果邻居恰好是噪
	声点，会导致过拟合。
	• 一般情况下，K 会倾向选取较小的值，并使用交叉验证法选取最优 K 值。

	决策树是一种树形结构的分类器，通过顺序询问分类点的属性决定分类点最终的类别。通常根据特征的信息增益或其他指标，构建一颗决策树。在分类时，只需要按照决策树中的结点依次进行判断，即可得到样本所属类别。
	决策树本质上是寻找一种对特征空间上的划分，旨在构建一个训练数据拟合的好，并且复杂度小的决策树。
	
	朴素贝叶斯分类器是一个以贝叶斯定理为基础的多分类的分类器。对于给定数据，首先基于特征的条件独立性假设，学习输入输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
	朴素贝叶斯是典型的生成学习方法，由训练数据学习联合概率分布，并求得后验概率分布。朴素贝叶斯一般在小规模数据上的表现很好，适合进行多分类任务。

2、先通过panda.read_table读入train_data.txt文件，需注意分隔符为空格。将前54列数据生成为feature；将55列数据生成为label，并返回feature和label。
3、通过panda.read_table读入test_data.txt文件，需注意分隔符为空格。将前54列数据生成为feature。
4、使用Imputer函数，通过设定strategy参数为‘mean’，使用平均值对缺失数据进行补全。fit()函数用于训练预处理器，transform()函数用于生成预处理结果。

针对题目要求作如下二步操作：
第一步：利用含有标签的训练用数据进行训练和测试。
由于测试数据没有标签，所以先用训练数据进行划分训练数据和测试数据。
5、使用train_test_split(x, y, test_size=0.3, random_state=0)函数，通过设置测试集比例test_size为0.3，将数据随机打乱，便于后续分类器的初始化和训练。
6、分别采用K近邻分类器(KNN)，决策树(DecisionTree)，朴素贝叶斯(GussianNB)三种分类器的初始化和训练
7、然后进行预测和分析获得如下结果：

The classification report for knn:
             precision    recall  f1-score   support

          1       0.96      0.96      0.96     44527
          2       0.96      0.97      0.96     59469
          3       0.93      0.96      0.95      7542
          4       0.90      0.76      0.82       574
          5       0.89      0.84      0.87      1982
          6       0.92      0.88      0.90      3645
          7       0.97      0.96      0.96      4274

avg / total       0.96      0.96      0.96    122013
---------------------------------------------------------

The classification report for DT:
             precision    recall  f1-score   support

          1       0.92      0.92      0.92     44527
          2       0.93      0.93      0.93     59469
          3       0.91      0.91      0.91      7542
          4       0.81      0.78      0.80       574
          5       0.77      0.78      0.78      1982
          6       0.83      0.84      0.83      3645
          7       0.93      0.93      0.93      4274

avg / total       0.92      0.92      0.92    122013
---------------------------------------------------------

The classification report for Bayes:
             precision    recall  f1-score   support

          1       0.50      0.78      0.61     44527
          2       0.84      0.16      0.27     59469
          3       0.44      0.81      0.57      7542
          4       0.22      0.82      0.34       574
          5       0.08      0.62      0.14      1982
          6       0.37      0.08      0.14      3645
          7       0.37      0.82      0.51      4274

avg / total       0.65      0.46      0.41    122013
---------------------------------------------------------

通过The classification report可以看出knn效果最好，DecisionTree效果也比较好，Bayes效果最差。

第二步：利用含有标签的训练用数据重新进行初始化和训练，再用测试用的数据进行预测。
8、重新通过panda.read_table读入train_data和test_data。
8、使用train_test_split()函数，通过设置测试集比例test_size为0，将数据随机打乱，便于后续分类器的初始化和训练。
9、通过“K近邻分类器(KNN)”对数据进行训练，再通过对test_x进行预测，并将获得预测结果存入model_1.txt。
10、通过“决策树(DecisionTree)”对数据进行训练，再通过对test_x进行预测，并将获得预测结果存入model_2.txt。
11、通过“朴素贝叶斯(GussianNB)”对数据进行训练，再通过对test_x进行预测，并将获得预测结果存入model_3.txt。
